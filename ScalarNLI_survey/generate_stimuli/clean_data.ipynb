{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import string\n",
    "pd.set_option('max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('interesting_sentences.txt', 'r', encoding = 'utf-8')\n",
    "next_sentence = \"\"\n",
    "last_sentences = []\n",
    "current_sentences = []\n",
    "phrases = []\n",
    "interesting_data = data.read()\n",
    "interesting_data = interesting_data.split('.')\n",
    "interesting_phrases = [\" good but not great\", \" big but not large\", \" huge but not large\",\" huge but not large\",\" small but not little\",\" interesting but not fascinating\"\n",
    "                       ,\" fat but not overweight\",\" good but not best\",\" great but not best\",\" strange but not weird\",\" known but not famous\",\" unusual but not strange\"\n",
    "                       ,\" possible but not practical\",\" wrong but not evil\",\" uncommon but not rare\",\" small but not tiny\",\" thin but not skinny\",\" known but not legendary\",\n",
    "                       \" dim but not dark\",\" sufficient but not ample\",\" interesting but not exciting\",\" further but not far\",\" uncommon but not unusual\",\" uncommon but not extraordinary\"\n",
    "                       ,\" bleak but not hopeless\" ,\" clean but not spotless\",\" unfortunate but not fatal\",\" unfortunate but not disastrous\",\" uncomfortable but not painful\",\n",
    "                       \" fat but not overweight\",\" neglected but not forgotten\",\" alarming but not terrifying\",\" big but not huge\",\" thick but not impenetrable\"]\n",
    "for i in range(2, len(interesting_data)):\n",
    "    \n",
    "    last_sentence = interesting_data[i - 2] + '. ' + interesting_data[i - 1]\n",
    "    current_sentence = interesting_data[i]\n",
    "    \n",
    "    cleaned_sentence = current_sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    for phrase in interesting_phrases:\n",
    "        if phrase in cleaned_sentence:\n",
    "            last_sentences.append(last_sentence.replace('\\n', ''))\n",
    "            current_sentences.append(current_sentence.replace('\\n', ''))\n",
    "            phrases.append(phrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data = [last_sentences, current_sentences, phrases])\n",
    "data = data.T\n",
    "data.columns = ['context', 'premise', 'phrase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data.premise[i] = data.premise[i].replace('\\'', '')\n",
    "    data.premise[i] = data.premise[i].replace('\\\"', '')\n",
    "    data.context[i] = data.context[i].replace('\\'', '')\n",
    "    data.context[i] = data.context[i].replace('\\\"', '')\n",
    "    split_sentence = data.premise[i].split(' ')\n",
    "    for j in range(len(split_sentence)  - 4):\n",
    "        if split_sentence[j] == 'very' and split_sentence[j+2] == 'but' and split_sentence[j+3] == 'not':\n",
    "            copy_list = copy.deepcopy(split_sentence)\n",
    "            del copy_list[j]\n",
    "            data.premise[i] = ' '.join(copy_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hypothesis'] = data['premise']\n",
    "remove_index = []\n",
    "for i in range(len(data)):\n",
    "    counter = 0\n",
    "    split_sentence = data.premise[i].split(' ')\n",
    "    hypothesis = copy.deepcopy(split_sentence)\n",
    "    premise = copy.deepcopy(hypothesis)\n",
    "    phrase = data.phrase[i][1:].split(' ')\n",
    "    j = -1\n",
    "    while j < len(hypothesis) - 3:\n",
    "        j = j + 1\n",
    "        if (hypothesis[j].translate(str.maketrans('', '', string.punctuation)) and hypothesis[j + 1].translate(str.maketrans('', '', string.punctuation)) == phrase[1] and \n",
    "            hypothesis[j + 2].translate(str.maketrans('', '', string.punctuation)) == phrase[2] and hypothesis[j + 3].translate(str.maketrans('', '', string.punctuation)) == phrase[3]):\n",
    "            \n",
    "            del hypothesis[j]\n",
    "            del hypothesis[j]\n",
    "            del hypothesis[j]\n",
    "            \n",
    "            del premise[j + 1]\n",
    "            del premise[j + 1]\n",
    "            del premise[j + 1]\n",
    "            counter = 1\n",
    "    if counter == 0:\n",
    "        remove_index.append(i)\n",
    "    data['hypothesis'][i] = ' '.join(hypothesis)\n",
    "    data['premise'][i] = ' '.join(premise)\n",
    "data = data[['context', 'premise', 'hypothesis', 'phrase']]\n",
    "data = data.drop(index=remove_index)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac = 1)\n",
    "\n",
    "\n",
    "phrases = [' good but not great', ' uncomfortable but not painful', ' small but not tiny', ' uncommon but not rare', ' good but not best', ' big but not huge', ' possible but not practical', ' dim but not dark',\n",
    "           ' clean but not spotless', ' interesting but not exciting', ' thin but not skinny', ' uncommon but not unusual', ' neglected but not forgotten', ' interesting but not fascinating', ' unfortunate but not fatal',\n",
    "           ' further but not far', ' known but not famous', ' unusual but not strange', ' big but not large', ' thick but not impenetrable', ' great but not best', ' unfortunate but not disastrous', ' bleak but not hopeless',\n",
    "           ' fat but not overweight', ' strange but not weird', ' small but not little', ' sufficient but not ample', ' alarming but not terrifying', ' uncommon but not extraordinary']\n",
    "test_count = [131, 92, 88, 76, 67, 65, 61, 26, 24, 24, 21, 20, 19, 17, 12, 9, 8, 6, 7, 6, 5, 4, 3, 2, 2, 2, 1, 1, 1]\n",
    "train_count = [600, 80, 80, 40]\n",
    "\n",
    "test_data = pd.DataFrame([], columns= ['context', 'premise', 'hypothesis', 'phrase'])\n",
    "train_data = pd.DataFrame([], columns= ['context', 'premise', 'hypothesis',  'phrase'])\n",
    "\n",
    "for i in range(len(test_count)):\n",
    "    if len(data[data['phrase'] == phrases[i]]) == 0:\n",
    "        print (phrases[i])\n",
    "    test_data = pd.concat([test_data, data[data['phrase'] == phrases[i]][:test_count[i]]])\n",
    "    \n",
    "for i in range(len(train_count)):\n",
    "    train_data = pd.concat([train_data, data[data['phrase'] == phrases[i]][test_count[i]:test_count[i] + train_count[i]]])\n",
    "\n",
    "train_data = train_data.sample(frac = 1)\n",
    "test_data = test_data.sample(frac = 1)\n",
    "train_data.to_csv('train_data.csv', index= False, sep=';')\n",
    "test_data.to_csv('test_data.csv', index= False, sep = ';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (tags/v3.9.12:b28265d, Mar 23 2022, 23:52:46) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ddf133b4eda059986521323f3292d266b4e97de12a5e96c4c9cf48956d966ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
